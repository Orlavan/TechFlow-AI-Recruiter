{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TechFlow AI Recruiter - Performance Evaluation\n",
    "\n",
    "This notebook evaluates the Multi-Agent System performance using labeled conversation data.\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **Accuracy**: Overall correct predictions\n",
    "- **Precision/Recall/F1**: Per-class performance\n",
    "- **Confusion Matrix**: Detailed prediction breakdown\n",
    "\n",
    "## Dataset:\n",
    "- `sms_conversations.json`: Real SMS conversations with labeled actions (CONTINUE, SCHEDULE, END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1b",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversations dataset\n",
    "with open('../sms_conversations.json', 'r', encoding='utf-8') as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations.\")\n",
    "\n",
    "# Count total labeled turns\n",
    "total_labels = sum(1 for conv in conversations for turn in conv['turns'] if turn.get('label'))\n",
    "print(f\"Total labeled turns: {total_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2b",
   "metadata": {},
   "source": [
    "## 2. Initialize Main Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.modules.agents import MainAgent\n",
    "\n",
    "# Initialize the Main Agent\n",
    "agent = MainAgent()\n",
    "print(\"Main Agent initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3b",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation\n",
    "\n",
    "Note: This may take a while due to API calls. Adjust `max_conversations` to limit for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_CONVERSATIONS = 10  # Set to None for all conversations\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "results = []\n",
    "\n",
    "test_convs = conversations[:MAX_CONVERSATIONS] if MAX_CONVERSATIONS else conversations\n",
    "print(f\"Evaluating {len(test_convs)} conversations...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for conv_idx, conv in enumerate(test_convs):\n",
    "    history = \"\"\n",
    "    \n",
    "    for turn in conv['turns']:\n",
    "        speaker = \"Recruiter\" if turn['speaker'] == 'recruiter' else \"Candidate\"\n",
    "        text = turn['text']\n",
    "        \n",
    "        # Only evaluate labeled recruiter turns\n",
    "        if turn['speaker'] == 'recruiter' and turn.get('label'):\n",
    "            try:\n",
    "                action = agent.decide_action(history)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                action = \"ERROR\"\n",
    "            \n",
    "            pred = action.lower()\n",
    "            true = turn['label'].lower()\n",
    "            \n",
    "            y_pred.append(pred)\n",
    "            y_true.append(true)\n",
    "            \n",
    "            results.append({\n",
    "                'conv_id': conv.get('conversation_id', conv_idx),\n",
    "                'turn_id': turn.get('turn_id'),\n",
    "                'predicted': pred,\n",
    "                'actual': true,\n",
    "                'correct': pred == true,\n",
    "                'text': text[:60]\n",
    "            })\n",
    "            \n",
    "            status = '✓' if pred == true else '✗'\n",
    "            print(f\"{status} Pred: {pred:10} | True: {true:10}\")\n",
    "        \n",
    "        history += f\"{speaker}: {text}\\n\"\n",
    "    \n",
    "    if (conv_idx + 1) % 5 == 0:\n",
    "        print(f\"--- Processed {conv_idx + 1}/{len(test_convs)} conversations ---\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4b",
   "metadata": {},
   "source": [
    "## 4. Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"=\"*50)\n",
    "print(f\"OVERALL ACCURACY: {accuracy:.2%}\")\n",
    "print(f\"Correct: {sum(r['correct'] for r in results)} / {len(results)}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5b",
   "metadata": {},
   "source": [
    "## 5. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Classification Report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6b",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Confusion Matrix\n",
    "labels = ['continue', 'schedule', 'end']\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    annot_kws={'size': 16}\n",
    ")\n",
    "plt.title('Confusion Matrix - Main Agent Decisions', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion matrix saved to 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7b",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Show errors\n",
    "errors_df = results_df[~results_df['correct']]\n",
    "print(f\"Total Errors: {len(errors_df)} / {len(results_df)}\")\n",
    "print(\"\\nError Examples:\")\n",
    "errors_df[['conv_id', 'predicted', 'actual', 'text']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution by type\n",
    "if len(errors_df) > 0:\n",
    "    error_types = errors_df.groupby(['actual', 'predicted']).size().reset_index(name='count')\n",
    "    error_types = error_types.sort_values('count', ascending=False)\n",
    "    print(\"\\nError Distribution:\")\n",
    "    print(error_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9b",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy\n",
    "print(\"\\nPER-CLASS ACCURACY\")\n",
    "print(\"=\"*50)\n",
    "for label in labels:\n",
    "    mask = results_df['actual'] == label\n",
    "    if mask.sum() > 0:\n",
    "        class_acc = results_df[mask]['correct'].mean()\n",
    "        print(f\"{label.upper():12} : {class_acc:.2%} ({mask.sum()} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Actual distribution\n",
    "actual_counts = pd.Series(y_true).value_counts().reindex(labels, fill_value=0)\n",
    "axes[0].bar(actual_counts.index, actual_counts.values, color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Actual Label Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Predicted distribution  \n",
    "pred_counts = pd.Series(y_pred).value_counts().reindex(labels, fill_value=0)\n",
    "axes[1].bar(pred_counts.index, pred_counts.values, color='coral', edgecolor='black')\n",
    "axes[1].set_title('Predicted Label Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('label_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11b",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "print(\"Results saved to 'evaluation_results.csv'\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Predictions: {len(y_true)}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Correct: {sum(results_df['correct'])}\")\n",
    "print(f\"Errors: {len(errors_df)}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
